{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-weight:bold; font-size:40px; color:green; line-height:1; margin:0px\">\n",
    "    Smart City Applications in Land Use and Transport (SCALUT)\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfNSW GTFS-R Bus Vehicle Positions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-weight:bold; font-size:24px; color:Gold; line-height:1; margin:4px 0px\">\n",
    "    1. Convert .PB.GZ to .CSV Files\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-weight:bold; font-size:18px; color:gray; line-height:1; margin:4px 0px\">\n",
    "    (Optional) Housekeeping: Install Libraries/Packages\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pytz\n",
    "# pip install protobuf\n",
    "# pip install pandas\n",
    "# pip install numpy\n",
    "\n",
    "# conda install pip\n",
    "# pip install flat-table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-weight:bold; font-size:18px; color:tomato; line-height:1; margin:4px 0px\">\n",
    "    Housekeeping: Import Libraries/Packages\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import fnmatch\n",
    "import tfnsw_gtfs_realtime_pb2  # TfNSW specific proto file is required in this directory \n",
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "from google.protobuf.json_format import MessageToJson\n",
    "import flat_table\n",
    "from datetime import datetime\n",
    "import csv\n",
    "from pytz import timezone\n",
    "import pytz\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-weight:bold; font-size:18px; color:tomato; line-height:1; margin:4px 0px\">\n",
    "    Specify Project Directory and Folders and Define Variables\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specify the folder that stores the .PB.GZ files to be processed\n",
    "# FileTP = '200901_0000-2400'\n",
    "# FileTP = '200930_0000-2400'\n",
    "# FileTP = '201014_0000-2400'\n",
    "# FileTP = '201111_0000-2400'\n",
    "# FileTP = '201202_0000-2400'\n",
    "FileTP = 'Test_201014_0800-0805'\n",
    "\n",
    "## Specify the GTFS-R file prefix\n",
    "GTFS_VP_Prefix = 'GTFS_VP'\n",
    "\n",
    "## Specifiy the main directory that stores input and output folders\n",
    "DataDir = r'C:\\OneMetis Dropbox\\@One.IMS\\Datasets\\SCALUT_DW\\TfNSW_GTFS_Buses'\n",
    "\n",
    "## Specifiy the main folders that stores input and output data\n",
    "FldRawPB = '10_Raw_PB'\n",
    "FldRawCSVvp = '11_CSV_Raw_VP'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-weight:bold; font-size:18px; color:tomato; line-height:1; margin:4px 0px\">\n",
    "    Read and Process .PB.GZ Files (VP)\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Directory Path\n",
    "DirRawPB = DataDir + '/' + FldRawPB + '/' + FileTP\n",
    "DirRawCSVvp = DataDir + '/' + FldRawCSVvp + '/' + FileTP\n",
    "if not os.path.exists(DirRawCSVvp):\n",
    "    os.makedirs(DirRawCSVvp)\n",
    "\n",
    "## Define File Name and Path for CSV Summary File\n",
    "CSVsummaryVP = 'Summary_VP_' + FileTP\n",
    "FilePathCSVsumVP = DirRawCSVvp + '/' + CSVsummaryVP + '.csv'\n",
    "## Check if compare file exists. Remove if exist.\n",
    "if os.path.exists(FilePathCSVsumVP):\n",
    "    os.remove(FilePathCSVsumVP)\n",
    "\n",
    "utc = pytz.utc\n",
    "feed = tfnsw_gtfs_realtime_pb2.FeedMessage()\n",
    "\n",
    "## Record Start Time\n",
    "tStart = datetime.now()\n",
    "\n",
    "iFile = 0\n",
    "\n",
    "with open(FilePathCSVsumVP, mode='w', newline='') as csv_file:\n",
    "    FieldNames = ['FileName','Entity','FileTS','Header.TS','Header.GTFSRversion','Header.Incrementality','Record']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=FieldNames)\n",
    "    writer.writeheader()\n",
    "    \n",
    "    ## Looping .PB.GZ files saved in DirRawPB\n",
    "    all_Raw = glob.glob(os.path.join(DirRawPB, GTFS_VP_Prefix + '*.pb.gz'))\n",
    "    for FilePathRaw in all_Raw:\n",
    "        ## Get FullFileName from Path\n",
    "        strFullFileName = FilePathRaw.split('\\\\')[-1]\n",
    "        ##print(strFullFileName)\n",
    "\n",
    "        ## FileName exclude Extension\n",
    "        FNexExt = strFullFileName[0:-6]\n",
    "        ## Timestamp from FileName\n",
    "        fileTS = FNexExt[-16:]\n",
    "\n",
    "        ## Open .PB.GZ file\n",
    "        with gzip.open(FilePathRaw) as f:\n",
    "            feed.ParseFromString(f.read())\n",
    "\n",
    "        # ## OPTIONAL: DISPLAY HEADER DATA\n",
    "        # HeaderData = feed.header\n",
    "        # HeaderData\n",
    "\n",
    "        ## ENTITY DATA\n",
    "        entitylist = [entity for entity in feed.entity]\n",
    "        # print(len(entitylist))\n",
    "\n",
    "        # ## OPTIONAL: DISPLAY ENTITY DATA\n",
    "        # entitylist[0]\n",
    "\n",
    "        ## Protobof to JSON\n",
    "        jsonObj = MessageToJson(feed,preserving_proto_field_name=True)\n",
    "        data = json.loads(jsonObj)\n",
    "\n",
    "        ## Normalise JSON to Dataframe\n",
    "        df_js = json_normalize (data)\n",
    "        ## Get Header.Timestamp Value\n",
    "        HeaderGTFSver = df_js._get_value(0,'header.gtfs_realtime_version')\n",
    "        HeaderIncr = df_js._get_value(0,'header.incrementality')\n",
    "        HeaderTS = df_js._get_value(0,'header.timestamp')\n",
    "\n",
    "        ## Convert 'headerTS' from str to int \n",
    "        iHeaderTS = int(HeaderTS)\n",
    "        ## Convert 'headerTS' from POSIX Time to UTC Time\n",
    "        UTC_HeaderTS = utc.localize(datetime.utcfromtimestamp(iHeaderTS)).astimezone(timezone('Australia/Sydney')).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        ## JSON 'Entity' to Dataframe\n",
    "        df_VP = json_normalize (data, 'entity')\n",
    "        df_VPft = flat_table.normalize(df_VP)\n",
    "        # df_VPft.head(2)\n",
    "\n",
    "        ## Drop Columns\n",
    "        df_GTFS_VP = df_VPft.drop(['index', \n",
    "                                   'vehicle.vehicle.[transit_realtime.tfnsw_vehicle_descriptor].air_conditioned', \n",
    "                                   'vehicle.vehicle.[transit_realtime.tfnsw_vehicle_descriptor].wheelchair_accessible', \n",
    "                                   'vehicle.vehicle.[transit_realtime.tfnsw_vehicle_descriptor].vehicle_model', \n",
    "                                   'vehicle.vehicle.[transit_realtime.tfnsw_vehicle_descriptor].performing_prior_trip', \n",
    "                                   'vehicle.vehicle.[transit_realtime.tfnsw_vehicle_descriptor].special_vehicle_attributes'], \n",
    "                                  1)\n",
    "        \n",
    "        ## Sort Columns\n",
    "        df_GTFS_VP = df_GTFS_VP[['id',\n",
    "                                 'vehicle.trip.trip_id',\n",
    "                                 'vehicle.trip.start_time',\n",
    "                                 'vehicle.trip.start_date',\n",
    "                                 'vehicle.trip.schedule_relationship',\n",
    "                                 'vehicle.trip.route_id',\n",
    "                                 'vehicle.position.latitude',\n",
    "                                 'vehicle.position.longitude',\n",
    "                                 'vehicle.position.bearing',\n",
    "                                 'vehicle.position.speed',\n",
    "                                 'vehicle.timestamp', \n",
    "                                 'vehicle.congestion_level',\n",
    "                                 'vehicle.vehicle.id',\n",
    "#                                  'vehicle.vehicle.[transit_realtime.tfnsw_vehicle_descriptor].air_conditioned',\n",
    "#                                  'vehicle.vehicle.[transit_realtime.tfnsw_vehicle_descriptor].wheelchair_accessible',\n",
    "#                                  'vehicle.vehicle.[transit_realtime.tfnsw_vehicle_descriptor].vehicle_model',\n",
    "#                                  'vehicle.vehicle.[transit_realtime.tfnsw_vehicle_descriptor].performing_prior_trip',\n",
    "#                                  'vehicle.vehicle.[transit_realtime.tfnsw_vehicle_descriptor].special_vehicle_attributes',\n",
    "                                 'vehicle.occupancy_status']]\n",
    "\n",
    "        ## Add New Columns\n",
    "        df_GTFS_VP['VPheaderTS'] = UTC_HeaderTS\n",
    "        df_GTFS_VP['vehicle.timestampUTC'] = pd.DatetimeIndex(pd.to_datetime(df_GTFS_VP['vehicle.timestamp'],unit='s'),\n",
    "                                                              tz='UTC').tz_convert('Australia/Sydney').tz_localize(None)\n",
    "        df_GTFS_VP['vehicle.trip.start_DateTimeUTC'] = pd.to_datetime(df_GTFS_VP['vehicle.trip.start_date'],\n",
    "                                                                      format='%Y%m%d') + pd.to_timedelta(\n",
    "            df_GTFS_VP['vehicle.trip.start_time'])\n",
    "        # df_GTFS_VP.head(2)\n",
    "\n",
    "        ## Output to CSV\n",
    "        df_GTFS_VP.to_csv(DirRawCSVvp + '/' + FNexExt + '.csv', index=False)\n",
    "\n",
    "        ## Write data to CSV Summary\n",
    "        writer.writerow({'FileName':strFullFileName, \n",
    "                         'Entity':len(entitylist), \n",
    "                         'FileTS':fileTS, \n",
    "                         'Header.TS':UTC_HeaderTS, \n",
    "                         'Header.GTFSRversion':HeaderGTFSver, \n",
    "                         'Header.Incrementality':HeaderIncr, \n",
    "                         'Record':df_GTFS_VP.shape[0]})\n",
    "\n",
    "        ## Count File\n",
    "        iFile = iFile + 1\n",
    "        \n",
    "## Record End Time\n",
    "tEnd = datetime.now()\n",
    "print(iFile, 'Files Completed:', tEnd.isoformat(' ', 'seconds') + '; Time Spent:', tEnd-tStart)\n",
    "print('CSV Summary file saved in:', FilePathCSVsumVP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "GTFS_r_test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
