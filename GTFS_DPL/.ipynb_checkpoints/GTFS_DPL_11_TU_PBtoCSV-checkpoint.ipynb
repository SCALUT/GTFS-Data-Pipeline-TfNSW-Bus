{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-weight:bold; font-size:40px; color:green; line-height:1; margin:0px\">\n",
    "    Smart City Applications in Land Use and Transport (SCALUT)\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfNSW GTFS-R Bus Trip Update "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-weight:bold; font-size:24px; color:Gold; line-height:1; margin:4px 0px\">\n",
    "    1.1 Convert .PB.GZ to .CSV Files\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-weight:bold; font-size:18px; color:gray; line-height:1; margin:4px 0px\">\n",
    "    (Optional) Housekeeping: Install Libraries/Packages\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install pytz\n",
    "# pip install protobuf\n",
    "# pip install pandas\n",
    "# pip install numpy\n",
    "\n",
    "# conda install pip\n",
    "# pip install flat-table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-weight:bold; font-size:18px; color:tomato; line-height:1; margin:4px 0px\">\n",
    "    Housekeeping: Import Libraries/Packages\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "# import fnmatch\n",
    "import tfnsw_gtfs_realtime_pb2  # TfNSW specific proto file is required in this directory \n",
    "import gzip\n",
    "import json\n",
    "import pandas as pd\n",
    "from pandas import json_normalize\n",
    "from google.protobuf.json_format import MessageToJson\n",
    "import flat_table\n",
    "from datetime import datetime\n",
    "import csv\n",
    "from pytz import timezone\n",
    "import pytz\n",
    "import glob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-weight:bold; font-size:18px; color:tomato; line-height:1; margin:4px 0px\">\n",
    "    Specify Project Directory and Folders and Define Variables\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specify the folder that stores the .PB.GZ files to be processed\n",
    "# FileTP = '200901_0000-2400'\n",
    "# FileTP = '200930_0000-2400'\n",
    "# FileTP = '201014_0000-2400'\n",
    "# FileTP = '201111_0000-2400'\n",
    "# FileTP = '201202_0000-2400'\n",
    "FileTP = '2020m06d04'\n",
    "\n",
    "## Specify the GTFS-R file prefix\n",
    "GTFS_TU_Prefix = 'GTFS_TU'\n",
    "\n",
    "## Specifiy the main directory that stores input and output folders\n",
    "DataDir = r'C:\\OneMetis Dropbox\\@One.IMS\\Datasets\\SCALUT_DW\\TfNSW_GTFS_Buses'\n",
    "\n",
    "## Specifiy the main folders that stores input and output data\n",
    "FldRawPB = '10_Raw_PB'\n",
    "FldRawCSVtu = '11_CSV_Raw_TU'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-weight:bold; font-size:18px; color:tomato; line-height:1; margin:4px 0px\">\n",
    "    Read and Process .PB.GZ Files (TU)\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Directory Path\n",
    "DirRawPB = DataDir + '/' + FldRawPB + '/' + FileTP\n",
    "DirRawCSVtu = DataDir + '/' + FldRawCSVtu + '/' + FileTP\n",
    "if not os.path.exists(DirRawCSVtu):\n",
    "    os.makedirs(DirRawCSVtu)\n",
    "\n",
    "## Define File Name and Path for Comparison File\n",
    "CSVsummaryTU = 'Summary_TU_' + FileTP\n",
    "FilePathCSVsumTU = DirRawCSVtu + '/' + CSVsummaryTU + '.csv'\n",
    "## Check if compare file exists. Remove if exist.\n",
    "if os.path.exists(FilePathCSVsumTU):\n",
    "    os.remove(FilePathCSVsumTU)\n",
    "\n",
    "utc = pytz.utc\n",
    "feed = tfnsw_gtfs_realtime_pb2.FeedMessage()\n",
    "\n",
    "## Record Start Time\n",
    "tStart = datetime.now()\n",
    "\n",
    "iFile = 0\n",
    "\n",
    "with open(FilePathCSVsumTU, mode='w', newline='') as csv_file:\n",
    "    FieldNames = ['FileName','Entity','FileTS','Header.TS','Header.GTFSRversion','Header.Incrementality','Record']\n",
    "    writer = csv.DictWriter(csv_file, fieldnames=FieldNames)\n",
    "    writer.writeheader()\n",
    "\n",
    "    ## Looping .PB.GZ files saved in DirRawPB\n",
    "    all_Raw = glob.glob(os.path.join(DirRawPB, GTFS_TU_Prefix + '*.pb.gz'))\n",
    "    for FilePathRaw in all_Raw:\n",
    "        ## Get FullFileName from Path\n",
    "        strFullFileName = FilePathRaw.split('\\\\')[-1]\n",
    "        ##print(strFullFileName)\n",
    "\n",
    "        ## FileName exclude Extension\n",
    "        FNexExt = strFullFileName[0:-6]\n",
    "        ## Timestamp from FileName\n",
    "        fileTS = FNexExt[-16:]\n",
    "\n",
    "        ## Open .PB.GZ file\n",
    "        with gzip.open(FilePathRaw) as f:\n",
    "            feed.ParseFromString(f.read())\n",
    "\n",
    "        # ## OPTIONAL: DISPLAY HEADER DATA\n",
    "        # HeaderData = feed.header\n",
    "        # HeaderData\n",
    "\n",
    "        ## ENTITY DATA\n",
    "        entitylist = [entity for entity in feed.entity ]\n",
    "        ##print(len(entitylist))\n",
    "\n",
    "        # ## OPTIONAL: DISPLAY ENTITY DATA\n",
    "        # entitylist[0]\n",
    "\n",
    "        ## Protobof to JSON\n",
    "        jsonObj = MessageToJson(feed,preserving_proto_field_name=True)\n",
    "        data = json.loads(jsonObj)\n",
    "\n",
    "        ## Normalise JSON to Dataframe\n",
    "        df_js = json_normalize (data)\n",
    "        ## Get Header Information\n",
    "        HeaderGTFSver = df_js._get_value(0,'header.gtfs_realtime_version')\n",
    "        HeaderIncr = df_js._get_value(0,'header.incrementality')\n",
    "        HeaderTS = df_js._get_value(0,'header.timestamp')\n",
    "\n",
    "        ## Convert 'headerTS' from str to int \n",
    "        iHeaderTS = int(HeaderTS)\n",
    "        ## Convert 'headerTS' from POSIX Time to UTC Time\n",
    "        UTC_HeaderTS = utc.localize(datetime.utcfromtimestamp(iHeaderTS)).astimezone(timezone('Australia/Sydney')).strftime('%Y-%m-%d %H:%M:%S')\n",
    "\n",
    "        ## JSON 'Entity' to Dataframe\n",
    "        df_TU = json_normalize (data, 'entity')\n",
    "        df_TUft = flat_table.normalize(df_TU)\n",
    "        # df_TUft.head(2)\n",
    "\n",
    "        ## Sort Data by 'index','arrival.time','stop_sequence'\n",
    "        df_TUft.sort_values(by=['index',\n",
    "                                'trip_update.stop_time_update.arrival.time',\n",
    "                                'trip_update.stop_time_update.stop_sequence'],\n",
    "                           inplace=True)\n",
    "\n",
    "        ## Drop Columns\n",
    "        df_GTFS_TU = df_TUft.drop(['index'], 1)\n",
    "\n",
    "        ## Sort Columns\n",
    "        df_GTFS_TU = df_GTFS_TU[['id',\n",
    "                                 'trip_update.trip.trip_id',\n",
    "                                 'trip_update.trip.start_time',\n",
    "                                 'trip_update.trip.start_date',\n",
    "                                 'trip_update.trip.schedule_relationship',\n",
    "                                 'trip_update.trip.route_id',\n",
    "                                 'trip_update.vehicle.id',\n",
    "                                 'trip_update.timestamp',\n",
    "                                 'trip_update.stop_time_update.stop_sequence',\n",
    "                                 'trip_update.stop_time_update.arrival.delay',\n",
    "                                 'trip_update.stop_time_update.arrival.time',\n",
    "                                 'trip_update.stop_time_update.departure.delay',\n",
    "                                 'trip_update.stop_time_update.departure.time',\n",
    "                                 'trip_update.stop_time_update.stop_id',\n",
    "                                 'trip_update.stop_time_update.schedule_relationship']]\n",
    "\n",
    "        ## Add New Column\n",
    "        df_GTFS_TU['TUheaderTS'] = UTC_HeaderTS\n",
    "        df_GTFS_TU['trip_update.timestampUTC'] = pd.DatetimeIndex(\n",
    "            pd.to_datetime(df_GTFS_TU['trip_update.timestamp'],unit='s'),tz='UTC').tz_convert(\n",
    "            'Australia/Sydney').tz_localize(None)\n",
    "        df_GTFS_TU['trip_update.stop_time_update.arrival.timeUTC'] = pd.DatetimeIndex(\n",
    "            pd.to_datetime(df_GTFS_TU['trip_update.stop_time_update.arrival.time'],unit='s'),tz='UTC').tz_convert(\n",
    "            'Australia/Sydney').tz_localize(None)\n",
    "        df_GTFS_TU['trip_update.stop_time_update.departure.timeUTC'] = pd.DatetimeIndex(\n",
    "            pd.to_datetime(df_GTFS_TU['trip_update.stop_time_update.departure.time'],unit='s'),tz='UTC').tz_convert(\n",
    "            'Australia/Sydney').tz_localize(None)\n",
    "        df_GTFS_TU['trip_update.trip.start_DateTimeUTC'] = pd.to_datetime(df_GTFS_TU['trip_update.trip.start_date'],\n",
    "                                                                          format='%Y%m%d') + pd.to_timedelta(\n",
    "            df_GTFS_TU['trip_update.trip.start_time'])\n",
    "        # df_GTFS_TU.head(2)\n",
    "\n",
    "        ## Output to CSV\n",
    "        df_GTFS_TU.to_csv(DirRawCSVtu + '/' + FNexExt + '.csv', index=False)\n",
    "\n",
    "        ## Write data to CSV Summary\n",
    "        writer.writerow({'FileName':strFullFileName, \n",
    "                         'Entity':len(entitylist), \n",
    "                         'FileTS':fileTS, \n",
    "                         'Header.TS':UTC_HeaderTS, \n",
    "                         'Header.GTFSRversion':HeaderGTFSver, \n",
    "                         'Header.Incrementality':HeaderIncr, \n",
    "                         'Record':df_GTFS_TU.shape[0]})\n",
    "\n",
    "        ## Count File\n",
    "        iFile = iFile + 1\n",
    "        \n",
    "## Record End Time\n",
    "tEnd = datetime.now()\n",
    "print(iFile, 'Files Completed:', tEnd.isoformat(' ', 'seconds') + '; Time Spent:', tEnd-tStart)\n",
    "print('Timestamps comparison file saved in:', FilePathCSVsumTU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "GTFS_r_test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
