{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-weight:bold; font-size:40px; color:green; line-height:1; margin:0px\">\n",
    "    Smart City Applications in Land Use and Transport (SCALUT)\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfNSW GTFS-R Bus Trip Update "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-weight:bold; font-size:24px; color:Gold; line-height:1; margin:4px 0px\">\n",
    "    1.3 Prepare Cleaned Unique Datasets\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-weight:bold; font-size:18px; color:tomato; line-height:1; margin:4px 0px\">\n",
    "    Housekeeping: Import Libraries/Packages\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "# import glob\n",
    "import time\n",
    "from zipfile import ZipFile"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-weight:bold; font-size:18px; color:tomato; line-height:1; margin:4px 0px\">\n",
    "    Specify Project Directory and Folders and Define Variables\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specifiy the main directory that stores input and output folders\n",
    "DataDir = r'C:\\OneMetis Dropbox\\@One.IMS\\Datasets\\SCALUT_DW\\TfNSW_GTFS_Buses'\n",
    "\n",
    "## Specify the folder that stores the .PB.GZ files to be processed\n",
    "FileTP = '2020m06'\n",
    "DaysInMonth = 2\n",
    "# FileId = FileTP + 'd02'\n",
    "\n",
    "# ## Specify the GTFS-R file prefix\n",
    "GTFS_TU_Prefix = 'GTFS_TU'\n",
    "\n",
    "## Specifiy the main folders that stores GTFS Static data\n",
    "FldRawStatic = '10_Raw_Static'\n",
    "FileIdStatic = '20200601190600'\n",
    "# FileIdStatic = '20200901190900'\n",
    "# FileIdStatic = '20201001191000'\n",
    "# FileIdStatic = '20201014201000'\n",
    "\n",
    "## Specifiy the main folders that stores input and output data\n",
    "# FldRawPB = '10_Raw_PB'\n",
    "# FldRawCSVtu = '11_CSV_Raw_TU'\n",
    "# FldTransTU = '12_CSV_Transformed_TU'\n",
    "FldClnTU = '13_CSV_Cleaned_Unique_TU'\n",
    "FldClnTU_Agency = '13_CSV_Cleaned_Unique_TU_byAgency'\n",
    "\n",
    "## Specify the filename and location for the Routes List \n",
    "# FN_RoutesList = 'List_RouteShortNames_SydneyT80.txt'\n",
    "# FN_RoutesList = 'List_RouteShortNames_TheGong.txt'\n",
    "## OR\n",
    "Flt_Agency = 'Premier Illawarra'\n",
    "# Flt_Agency = 'Transit Systems'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Directory Path\n",
    "DirClnTU = DataDir + '/' + FldClnTU + '/' + FileTP\n",
    "if not os.path.exists(DirClnTU):\n",
    "    os.makedirs(DirClnTU)\n",
    "    \n",
    "DirClnTU_Agency = DataDir + '/' + FldClnTU_Agency + '/' + FileTP\n",
    "if not os.path.exists(DirClnTU_Agency):\n",
    "    os.makedirs(DirClnTU_Agency)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-weight:bold; font-size:18px; color:tomato; line-height:1; margin:4px 0px\">\n",
    "    Define Functions\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "## Read Cleaned Unique TU CSV\n",
    "def Read_CSV_Cln_TU(f):\n",
    "    df_CSV_Cln_TU = pd.read_csv(f, sep=',', dtype={'id':'str',\n",
    "                                                   'trip_update.trip.trip_id':'str',\n",
    "                                                   'trip_update.trip.start_time':'str',\n",
    "                                                   'trip_update.trip.start_date':'str',\n",
    "                                                   'trip_update.trip.schedule_relationship':'str',\n",
    "                                                   'trip_update.trip.route_id':'str',\n",
    "                                                   'trip_update.vehicle.id':'str',\n",
    "                                                   'trip_update.timestamp':'Int64',\n",
    "                                                   'trip_update.stop_time_update.stop_sequence':'Int64',\n",
    "                                                   'trip_update.stop_time_update.arrival.delay':'Int64',\n",
    "                                                   'trip_update.stop_time_update.arrival.time':'Int64',\n",
    "                                                   'trip_update.stop_time_update.departure.delay':'Int64',\n",
    "                                                   'trip_update.stop_time_update.departure.time':'Int64',\n",
    "                                                   'trip_update.stop_time_update.stop_id':'str',\n",
    "                                                   'trip_update.stop_time_update.schedule_relationship':'str',\n",
    "                                                   'TUheaderTS':'str',\n",
    "                                                   'trip_update.timestampUTC':'str',\n",
    "                                                   'trip_update.stop_time_update.arrival.timeUTC':'str',\n",
    "                                                   'trip_update.stop_time_update.departure.timeUTC':'str',\n",
    "                                                   'trip_update.trip.start_DateTimeUTC':'str',\n",
    "                                                   'Rt.Scheduled_Arrival.Time':'str',\n",
    "                                                   'Rt.Scheduled_Arrival.TimeUTC':'str',\n",
    "                                                   'trip_update.trip.trip_id2':'str',\n",
    "                                                   'Rt.Scheduled_Arrival.TimeUTC2':'str',\n",
    "                                                   'stop_sequence':'Int64',\n",
    "                                                   'shape_dist_traveled':'float'\n",
    "                                                  }, \n",
    "                                parse_dates=['trip_update.stop_time_update.arrival.timeUTC','trip_update.trip.start_DateTimeUTC']\n",
    "                               )\n",
    "    return(df_CSV_Cln_TU)\n",
    "\n",
    "############################################################################\n",
    "## Fill Empty Stop Sequence Using Existing Data AND Flag Bad Observations ##\n",
    "def Fill_Empty_StopSeq3(df, df_S):\n",
    "    df = df.astype({'stop_sequence':'float'})\n",
    "    df['stop_sequence'] = df.groupby(['trip_update.trip.route_id', \n",
    "                                      'trip_update.trip.trip_id', \n",
    "                                      'trip_update.trip.start_DateTimeUTC',\n",
    "                                      'trip_update.stop_time_update.stop_id'\n",
    "                                     ])['stop_sequence'].apply(lambda x:x.fillna(x.mean()))\n",
    "    df['stop_sequence'] = df['stop_sequence'].round(0).astype('Int64')\n",
    "\n",
    "    ## Get shape_dist_traveled from GTFS Static\n",
    "    df1 = pd.merge(df, \n",
    "                   df_S[['trip_id','stop_id','stop_sequence','shape_dist_traveled']],\n",
    "                   how='left',\n",
    "                   suffixes=('','_S'),\n",
    "                   left_on=['trip_update.trip.trip_id2','trip_update.stop_time_update.stop_id','stop_sequence'],\n",
    "                   right_on=['trip_id','stop_id','stop_sequence'])\n",
    "    df1['shape_dist_traveled'].fillna(df1['shape_dist_traveled_S'], inplace=True)\n",
    "    ## Drop Columns\n",
    "    df2 = df1.drop(columns=['trip_id','stop_id','shape_dist_traveled_S'])\n",
    "    ## Flag Bad Observations\n",
    "    df2.sort_values(by=['trip_update.trip.route_id', \n",
    "                        'trip_update.trip.trip_id',\n",
    "                        'trip_update.trip.start_DateTimeUTC',\n",
    "                        'stop_sequence',\n",
    "                        'trip_update.timestamp'\n",
    "                       ], inplace=True)\n",
    "\n",
    "    ## Bad when 300s gap (from TP to TP)\n",
    "    df2['Bad_Flag0'] = df2.groupby(['trip_update.trip.route_id', \n",
    "                                    'trip_update.trip.trip_id', \n",
    "                                    'trip_update.trip.start_DateTimeUTC', \n",
    "                                    'stop_sequence'])['trip_update.timestamp'].diff().ge(300).fillna(0)*1\n",
    "\n",
    "    df2['Bad_Flag1'] = df2.groupby(['trip_update.trip.route_id', \n",
    "                                    'trip_update.trip.trip_id', \n",
    "                                    'trip_update.trip.start_DateTimeUTC', \n",
    "                                    'stop_sequence'])['Bad_Flag0'].cumsum()\n",
    "\n",
    "    return(df2)\n",
    "\n",
    "#################################################\n",
    "## Clean Duplicate Data\n",
    "def Df_Remove_Duplicate(df_Dup):\n",
    "#     df_Dup.sort_values(by=['trip_update.trip.route_id',\n",
    "#                            'trip_update.trip.trip_id',\n",
    "#                            'trip_update.trip.start_DateTimeUTC',\n",
    "#                            'stop_sequence',\n",
    "#                            'trip_update.timestamp'\n",
    "#                           ], inplace=True)\n",
    "    df_Dup2 = df_Dup[df_Dup['Bad_Flag1'] == 0]\n",
    "    ## Drop Columns\n",
    "    df_Dup2 = df_Dup2.drop(columns=['Bad_Flag0','Bad_Flag1'])\n",
    "    df_Unique = df_Dup2.drop_duplicates(subset=['trip_update.trip.route_id',\n",
    "                                                'trip_update.trip.trip_id',\n",
    "                                                'trip_update.trip.start_DateTimeUTC',\n",
    "                                                'stop_sequence'\n",
    "                                               ], keep='last')\n",
    "    return(df_Unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-weight:bold; font-size:18px; color:tomato; line-height:1; margin:4px 0px\">\n",
    "    Get Information from GTFS Static (Routes List by Agency)\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "## Static Directory Path\n",
    "FileStaticZip = 'complete_gtfs_scheduled_data_' + FileIdStatic + '.zip'\n",
    "DirStaticZip = DataDir + '/' + FldRawStatic + '/' + FileStaticZip\n",
    "\n",
    "ZipStatic = ZipFile(DirStaticZip)\n",
    "df_StTimes = pd.read_csv(ZipStatic.open('stop_times.txt'),\n",
    "                         dtype={'trip_id':'str','arrival_time':'str','departure_time':'str','stop_id':'str',\n",
    "                                'stop_sequence':'Int32','stop_headsign':'str','pickup_type':'int','drop_off_type':'int',\n",
    "                                'shape_dist_traveled':'float','timepoint':'int','stop_note':'str'},\n",
    "                        )\n",
    "# df_StTimes.head(2)\n",
    "\n",
    "#############################################\n",
    "## Get List of Routes based on Agency Name ##\n",
    "df_Agency = pd.read_csv(ZipStatic.open('agency.txt'),dtype='unicode')\n",
    "df_Routes = pd.read_csv(ZipStatic.open('routes.txt'),dtype='unicode')\n",
    "df_Routes['RT_route_id'] = df_Routes['agency_id'] + '_' + df_Routes['route_short_name']\n",
    "\n",
    "df_RoutesAgency = pd.merge(df_Routes,\n",
    "                           df_Agency[['agency_id','agency_name']],\n",
    "                           how='left',\n",
    "                           suffixes=('','_Ag'),\n",
    "                           on=['agency_id'])\n",
    "\n",
    "df_RoutesAgency_Flt = df_RoutesAgency[df_RoutesAgency['agency_name'].isin([Flt_Agency])]\n",
    "# df_RoutesAgency_Flt.head(1)\n",
    "\n",
    "List_AgencyRtRoutes = df_RoutesAgency_Flt['RT_route_id']\n",
    "# List_AgencyRtRoutes\n",
    "\n",
    "df_RoutesAgency_FltRT = df_RoutesAgency_Flt[df_RoutesAgency_Flt['route_type'] == '700']\n",
    "# df_RoutesAgency_FltRT.head(1)\n",
    "\n",
    "List_AgencyRtRoutes_700 = df_RoutesAgency_FltRT['RT_route_id']\n",
    "# List_AgencyRtRoutes_700"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-weight:bold; font-size:18px; color:tomato; line-height:1; margin:4px 0px\">\n",
    "    FOR ARTEMIS: Combine Cleaned Datasets from TP1-TP6 to Daily\n",
    "    and Create Daily Datasets by Agency\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Record Start Time\n",
    "tStart = datetime.now()\n",
    "print('PROCESSING DATA FOR', FileTP, \"...\")\n",
    "print('Time Start:', tStart.isoformat(' ', 'seconds'))\n",
    "print('')\n",
    "\n",
    "for iDay in range(1, DaysInMonth+1):\n",
    "\n",
    "    FileId = FileTP + 'd' + (str(iDay).zfill(2))\n",
    "\n",
    "    ## Define Output File Path\n",
    "    PathTransTUrtCln = DirClnTU + '/' + GTFS_TU_Prefix + '_' + FileId + '_Cln.csv'\n",
    "\n",
    "    ## Check if file exists. Remove if exist.\n",
    "    if os.path.exists(PathTransTUrtCln):\n",
    "        os.remove(PathTransTUrtCln)    \n",
    "\n",
    "    df_Con = []\n",
    "    for iTP in range(1, 7):\n",
    "\n",
    "        ## Define Input File Path\n",
    "        PathClnTP = DirClnTU + '/' + GTFS_TU_Prefix + '_' + FileId + 'tp' + str(iTP) + '_Cln.csv'    \n",
    "\n",
    "        if iTP == 1:\n",
    "            ## Call function to read Cln TU CSV files\n",
    "            df_Con = Read_CSV_Cln_TU(PathClnTP)\n",
    "            print(FileId + 'TP' + str(iTP), df_Con.shape)\n",
    "        else:\n",
    "            ## Call function to read Cln TU CSV files\n",
    "            df_X = Read_CSV_Cln_TU(PathClnTP)\n",
    "            print(FileId + 'TP' + str(iTP), df_X.shape)\n",
    "\n",
    "            ## Combine records from df_Con_Flt and df_X_Flt\n",
    "            df_Con = pd.concat([df_Con, df_X], ignore_index=True)\n",
    "\n",
    "    ############################################\n",
    "    ## Daily Cleaned Unique Dataset by Agency ##\n",
    "\n",
    "    ## FOR LIST OF ROUTES BY AGENCY\n",
    "    Agency = Flt_Agency\n",
    "    Flt_Routes_Rt = List_AgencyRtRoutes_700\n",
    "\n",
    "    ## Define Output File Path by Agency and RT700\n",
    "    PathTransTUrtClnAgency = DirClnTU_Agency + '/' + GTFS_TU_Prefix + '_' + FileId + \"_\" + Agency + '_700_Cln.csv'\n",
    "\n",
    "    ## Check if file exists. Remove if exist.\n",
    "    if os.path.exists(PathTransTUrtClnAgency):\n",
    "        os.remove(PathTransTUrtClnAgency)\n",
    "\n",
    "    ## Grab the records associated with the Agency\n",
    "    df_Con_Flt = df_Con[df_Con['trip_update.trip.route_id'].isin(Flt_Routes_Rt)]\n",
    "\n",
    "    ## Fill Empty Stop Sequence etc Using Existing Data\n",
    "    df_Con_Flt1 = Fill_Empty_StopSeq3(df_Con_Flt, df_StTimes)\n",
    "\n",
    "    ## Clean Duplicate Data\n",
    "    df_Con_Cln_Flt = Df_Remove_Duplicate(df_Con_Flt1)\n",
    "\n",
    "    ## Export Agency Daily Cleaned Data to CSV\n",
    "    df_Con_Cln_Flt.to_csv(PathTransTUrtClnAgency, index=False)\n",
    "\n",
    "    print('Combined Dataset:', FileId, df_Con.shape)\n",
    "    print('Combined Dataset by Agency:', FileId, df_Con_Flt.shape)\n",
    "    print('Combined Unique Dataset by Agency RT700:', Agency, FileId, df_Con_Cln_Flt.shape)\n",
    "    print('')\n",
    "\n",
    "## Record End Time\n",
    "tEnd = datetime.now()\n",
    "print(iDay, 'Days Processed:', tEnd.isoformat(' ', 'seconds') + '; Time Spent:', tEnd-tStart)\n",
    "print('COMPLETED ON', datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# #####################################################################################################################\n",
    "# ## SUPERSEDED: Combine and Export Cleaned Unique Dataset to Daily + Create and Export Daily Cleaned Unique Dataset ##  \n",
    "# #####################################################################################################################\n",
    "\n",
    "# ## Record Start Time\n",
    "# tStart = datetime.now()\n",
    "# print('PROCESSING DATA FOR', FileTP, \"...\")\n",
    "# print('Time Start:', tStart.isoformat(' ', 'seconds'))\n",
    "# print('')\n",
    "\n",
    "# for iDay in range(1, DaysInMonth+1):\n",
    "\n",
    "#     FileId = FileTP + 'd' + (str(iDay).zfill(2))\n",
    "\n",
    "#     ## Define Output File Path\n",
    "#     PathTransTUrtCln = DirClnTU + '/' + GTFS_TU_Prefix + '_' + FileId + '_Cln.csv'\n",
    "\n",
    "#     ## Check if file exists. Remove if exist.\n",
    "#     if os.path.exists(PathTransTUrtCln):\n",
    "#         os.remove(PathTransTUrtCln)    \n",
    "\n",
    "#     df_Con = []\n",
    "#     for iTP in range(1, 7):\n",
    "\n",
    "#         ## Define Input File Path\n",
    "#         PathClnTP = DirClnTU + '/' + GTFS_TU_Prefix + '_' + FileId + 'tp' + str(iTP) + '_Cln.csv'    \n",
    "\n",
    "#         if iTP == 1:\n",
    "#             ## Call function to read Cln TU CSV files\n",
    "#             df_Con = Read_CSV_Cln_TU(PathClnTP)\n",
    "#             print(FileId + 'TP' + str(iTP), df_Con.shape)\n",
    "#         else:\n",
    "#             ## Call function to read Cln TU CSV files\n",
    "#             df_X = Read_CSV_Cln_TU(PathClnTP)\n",
    "#             print(FileId + 'TP' + str(iTP), df_X.shape)\n",
    "\n",
    "#             ## Combine records from df_Con_Flt and df_X_Flt\n",
    "#             df_Con = pd.concat([df_Con, df_X], ignore_index=True)\n",
    "\n",
    "#     print('Day', iDay, 'Combined Size:', df_Con.shape)\n",
    "#     print('Filling Empty Stop Sequence etc ...')\n",
    "#     ## Fill Empty Stop Sequence Using Existing Data ##\n",
    "#     df_Con1 = Fill_Empty_StopSeq3(df_Con, df_StTimes)\n",
    "\n",
    "#     print('Cleaning duplicate data ...')\n",
    "#     ## Clean Duplicate Data\n",
    "#     df_Con_Cln = Df_Remove_Duplicate(df_Con1)\n",
    "\n",
    "#     print('Exporting cleaned unique data ...')\n",
    "#     ## Export Cleaned Data to CSV\n",
    "#     df_Con_Cln.to_csv(PathTransTUrtCln, index=False)\n",
    "\n",
    "#     ############################################\n",
    "#     ## Daily Cleaned Unique Dataset by Agency ##\n",
    "\n",
    "#     ## FOR LIST OF ROUTES BY AGENCY\n",
    "#     Agency = Flt_Agency\n",
    "#     Flt_Routes_Rt = List_AgencyRtRoutes_700\n",
    "\n",
    "#     ## Define Output File Path by Agency and RT700\n",
    "#     PathTransTUrtClnAgency = DirClnTU_Agency + '/' + GTFS_TU_Prefix + '_' + FileId + \"_\" + Agency + '_700_Cln.csv'\n",
    "\n",
    "#     ## Check if file exists. Remove if exist.\n",
    "#     if os.path.exists(PathTransTUrtClnAgency):\n",
    "#         os.remove(PathTransTUrtClnAgency)\n",
    "\n",
    "#     print('Filtering cleaned unique data Agency and Route Type 700 ...')\n",
    "#     ## Grab the records associated with the Agency\n",
    "#     df_Con_Cln_Flt = df_Con_Cln[df_Con_Cln['trip_update.trip.route_id'].isin(Flt_Routes_Rt)]\n",
    "#     ## Export Agency Daily Cleaned Data to CSV\n",
    "#     df_Con_Cln_Flt.to_csv(PathTransTUrtClnAgency, index=False)\n",
    "\n",
    "#     print('Combined Dataset:', FileId, df_Con.shape)\n",
    "#     print('Combined Unique Dataset:', FileId, df_Con_Cln.shape)\n",
    "#     print('Combined Unique Dataset by Agency:', Agency, FileId, df_Con_Cln_Flt.shape)\n",
    "#     print('')\n",
    "\n",
    "# ## Record End Time\n",
    "# tEnd = datetime.now()\n",
    "# print(iDay, 'Days Processed:', tEnd.isoformat(' ', 'seconds') + '; Time Spent:', tEnd-tStart)\n",
    "# print('COMPLETED ON', datetime.now())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-weight:bold; font-size:18px; color:tomato; line-height:1; margin:4px 0px\">\n",
    "    FOR ARTEMIS: Combine Datasets by Agency from Daily to Monthly\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Record Start Time\n",
    "tStart = datetime.now()\n",
    "print('PROCESSING DATA FOR', FileTP, \"...\")\n",
    "print('Time Start:', tStart.isoformat(' ', 'seconds'))\n",
    "\n",
    "## Define Output File Path\n",
    "PathTransTUrtClnAgencyMonth = DirClnTU_Agency + '/' + GTFS_TU_Prefix + '_' + FileTP + '_' + Agency + '_700_Cln.csv'\n",
    "\n",
    "## Check if file exists. Remove if exist.\n",
    "if os.path.exists(PathTransTUrtClnAgencyMonth):\n",
    "    os.remove(PathTransTUrtClnAgencyMonth)\n",
    "\n",
    "df_ConAgency = []\n",
    "\n",
    "for iDay in range(1, DaysInMonth+1):\n",
    "\n",
    "    ## Define Input File Path\n",
    "    FileId = FileTP + 'd' + (str(iDay).zfill(2))\n",
    "    PathTransTUrtClnAgency = DirClnTU_Agency + '/' + GTFS_TU_Prefix + '_' + FileId + \"_\" + Agency + '_700_Cln.csv'\n",
    "\n",
    "    if iDay == 1:\n",
    "        ## Call function to read Cln TU CSV files\n",
    "        df_ConAgency = Read_CSV_Cln_TU(PathTransTUrtClnAgency)\n",
    "        print('Day' + str(iDay), df_ConAgency.shape)\n",
    "    else:\n",
    "        ## Call function to read Cln TU CSV files\n",
    "        df_XAgency = Read_CSV_Cln_TU(PathTransTUrtClnAgency)\n",
    "        print('Day' + str(iDay), df_XAgency.shape)\n",
    "\n",
    "        ## Combine records from df_Con_Flt and df_X_Flt\n",
    "        df_ConAgency = pd.concat([df_ConAgency, df_XAgency], ignore_index=True)\n",
    "\n",
    "        \n",
    "## Fill Empty Stop Sequence etc Using Existing Data\n",
    "df_ConAgency1 = Fill_Empty_StopSeq3(df_ConAgency, df_StTimes)\n",
    "\n",
    "## Clean Duplicate Data\n",
    "df_ConTU_ClnAgency = Df_Remove_Duplicate(df_ConAgency1)\n",
    "\n",
    "## Export Agency Monthly Cleaned Data to CSV\n",
    "df_ConTU_ClnAgency.to_csv(PathTransTUrtClnAgencyMonth, index=False)\n",
    "\n",
    "## Record End Time\n",
    "tEnd = datetime.now()\n",
    "print('TP' + str(iTP), 'Files Processed:', tEnd.isoformat(' ', 'seconds') + '; Time Spent:', tEnd-tStart)\n",
    "print('Combined Dataset:', df_ConAgency.shape)\n",
    "print('Combined Unique Dataset:', df_ConTU_ClnAgency.shape)\n",
    "print('COMPLETED ON', datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-weight:bold; font-size:18px; color:purple; line-height:1; margin:4px 0px\">\n",
    "    Get Information from GTFS Static (Filter Route Type)\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#############################################\n",
    "## Static Directory Path\n",
    "FileStaticZip = 'complete_gtfs_scheduled_data_' + FileIdStatic + '.zip'\n",
    "DirStaticZip = DataDir + '/' + FldRawStatic + '/' + FileStaticZip\n",
    "\n",
    "ZipStatic = ZipFile(DirStaticZip)\n",
    "df_StTimes = pd.read_csv(ZipStatic.open('stop_times.txt'),\n",
    "                         dtype={'trip_id':'str','arrival_time':'str','departure_time':'str','stop_id':'str',\n",
    "                                'stop_sequence':'Int32','stop_headsign':'str','pickup_type':'int','drop_off_type':'int',\n",
    "                                'shape_dist_traveled':'float','timepoint':'int','stop_note':'str'},\n",
    "                        )\n",
    "# df_StTimes.head(2)\n",
    "\n",
    "#############################################\n",
    "## Get List of Routes based on Agency Name ##\n",
    "df_Agency = pd.read_csv(ZipStatic.open('agency.txt'),dtype='unicode')\n",
    "df_Routes = pd.read_csv(ZipStatic.open('routes.txt'),dtype='unicode')\n",
    "df_Routes['RT_route_id'] = df_Routes['agency_id'] + '_' + df_Routes['route_short_name']\n",
    "\n",
    "df_RoutesAgency = pd.merge(df_Routes,\n",
    "                           df_Agency[['agency_id','agency_name']],\n",
    "                           how='left',\n",
    "                           suffixes=('','_Ag'),\n",
    "                           on=['agency_id'])\n",
    "\n",
    "df_RoutesAgency_Flt = df_RoutesAgency[df_RoutesAgency['agency_name'].isin([Flt_Agency])]\n",
    "# df_RoutesAgency_Flt.head(1)\n",
    "\n",
    "List_AgencyRtRoutes = df_RoutesAgency_Flt['RT_route_id']\n",
    "# List_AgencyRtRoutes\n",
    "\n",
    "df_RoutesAgency_FltRT = df_RoutesAgency_Flt[df_RoutesAgency_Flt['route_type'] == '700']\n",
    "# df_RoutesAgency_FltRT.head(1)\n",
    "\n",
    "List_AgencyRtRoutes_700 = df_RoutesAgency_FltRT['RT_route_id']\n",
    "List_AgencyRtRoutes_700"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-weight:bold; font-size:18px; color:gray; line-height:1; margin:4px 0px\">\n",
    "    TEMP: Summarise Datasets\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "ClnFile = r'C:\\OneMetis Dropbox\\@One.IMS\\Datasets\\SCALUT_DW\\TfNSW_GTFS_Buses\\13_CSV_Cleaned_Unique_TU_byAgency\\2020m10/GTFS_TU_2020m10_Premier Illawarra_Cln.csv'\n",
    "df_CSV_Cln_TU = pd.read_csv(ClnFile, sep=',', dtype='unicode')\n",
    "# df_CSV_Cln_TU.head(1)\n",
    "df_Routes = df_CSV_Cln_TU.groupby(['trip_update.trip.route_id']).agg({'trip_update.trip.trip_id':'count'}).reset_index()\n",
    "# df_Routes = df_CSV_Cln_TU.groupby(['trip_update.stop_time_update.departure.timeUTC']).agg({'trip_update.trip.trip_id':'count'}).reset_index()\n",
    "df_Routes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_CSV_Cln_TU.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-weight:bold; font-size:18px; color:gold; line-height:normal; margin:0px; padding:4px; background-color:black\">\n",
    "    Export Current Notebook to Other Format(s)\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from notebooktoall.transform import transform_notebook\n",
    "iPyNB = 'SCALUT_DPL_TfNSW_GTFS-R_Bus_13_TU_ClnUnique_v02_byAgency.ipynb\n",
    "transform_notebook(ipynb_file=iPyNB', export_list=['html', 'py'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to SCALUT TfNSW GTFS <b>[Table of Contents](SCALUT_TfNSW_GTFS_Analysis_TOC_v01.ipynb)</b>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "GTFS_r_test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
