{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-weight:bold; font-size:40px; color:green; line-height:1; margin:0px\">\n",
    "    Smart City Applications in Land Use and Transport (SCALUT)\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TfNSW GTFS-R Bus Trip Update "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-weight:bold; font-size:24px; color:Gold; line-height:1; margin:4px 0px\">\n",
    "    1.2 Transform .CSV Files\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-weight:bold; font-size:18px; color:tomato; line-height:1; margin:4px 0px\">\n",
    "    Housekeeping: Import Libraries/Packages\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from datetime import datetime\n",
    "import pandas as pd\n",
    "import glob\n",
    "import time\n",
    "from zipfile import ZipFile\n",
    "from GTFS_DPL_Funcs import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-weight:bold; font-size:18px; color:tomato; line-height:1; margin:4px 0px\">\n",
    "    Specify Project Directory and Folders and Define Variables\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specifiy the main directory that stores input and output folders\n",
    "DataDir = r'C:\\OneMetis Dropbox\\@One.IMS\\Datasets\\SCALUT_DW\\TfNSW_GTFS_Buses'\n",
    "\n",
    "## Specify the folder that stores the .PB.GZ files to be processed\n",
    "FileTP = 'Test_201014_0800-0805'\n",
    "DayInMonth = 4\n",
    "\n",
    "# ## Specify the GTFS-R file prefix\n",
    "GTFS_TU_Prefix = 'GTFS_TU'\n",
    "\n",
    "## Specifiy the main folders that stores input and output data\n",
    "# FldRawPB = '10_Raw_PB'\n",
    "FldRawCSVtu = '11_CSV_Raw_TU'\n",
    "FldTransTU = '12_CSV_Transformed_TU'\n",
    "FldClnTU = '13_CSV_Cleaned_Unique_TU'\n",
    "\n",
    "## Filter by Agency\n",
    "Flt_Agency = 'Premier Illawarra'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specifiy the main folders that stores GTFS Static data\n",
    "FldRawStatic = '10_Raw_Static'\n",
    "StaticIdLkUp = {\n",
    "#     'FileTP':'StaticId', \n",
    "    'Test_201014_0800-0805':'20201001191000', \n",
    "    '2020m06':'20200601190600', \n",
    "    '2020m07':'20200701190700', \n",
    "    '2020m08':'20200803190800', \n",
    "    '2020m09':'20200901190900', \n",
    "    '2020m10':'20201001191000', \n",
    "    '2020m11':'20201102191100', \n",
    "    '2020m12':'20201201191200', \n",
    "}\n",
    "if FileTP in StaticIdLkUp.keys():\n",
    "    FileIdStatic = StaticIdLkUp[FileTP]\n",
    "    print(FileIdStatic)\n",
    "else:\n",
    "    print(f\"ERROR: '{FileTP}' is not a key within the StaticIdLkUp.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Directory Path\n",
    "# DirRawPBtu = DataDir + '/' + FldRawPBtu + '/' + FileTP\n",
    "DirRawPBtu = os.path.join(DataDir, FldRawPBtu, FileTP)\n",
    "\n",
    "# DirRawCSVtu = DataDir + '/' + FldRawCSVtu + '/' + FileTP\n",
    "DirRawCSVtu = os.path.join(DataDir, FldRawCSVtu, FileTP)\n",
    "if not os.path.exists(DirRawCSVtu):\n",
    "    os.makedirs(DirRawCSVtu)\n",
    "\n",
    "# DirTransTU = DataDir + '/' + FldTransTU + '/' + FileTP\n",
    "DirTransTU = os.path.join(DataDir, FldTransTU, FileTP)\n",
    "if not os.path.exists(DirTransTU):\n",
    "    os.makedirs(DirTransTU)\n",
    "\n",
    "# DirClnTU = DataDir + '/' + FldClnTU + '/' + FileTP\n",
    "DirClnTU = os.path.join(DataDir, FldClnTU, FileTP)\n",
    "if not os.path.exists(DirClnTU):\n",
    "    os.makedirs(DirClnTU)\n",
    "\n",
    "# File_RoutesList = DataDir + '/' + FN_RoutesList"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-weight:bold; font-size:18px; color:tomato; line-height:1; margin:4px 0px\">\n",
    "    Define Functions\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "## Read Raw TU CSV\n",
    "def Read_CSV_Raw_TU(f):\n",
    "    df_CSV_Raw_TU = pd.read_csv(f, sep=',', dtype={'id':'str',\n",
    "                                                   'trip_update.trip.trip_id':'str',\n",
    "                                                   'trip_update.trip.start_time':'str',\n",
    "                                                   'trip_update.trip.start_date':'str',\n",
    "                                                   'trip_update.trip.schedule_relationship':'str',\n",
    "                                                   'trip_update.trip.route_id':'str',\n",
    "                                                   'trip_update.vehicle.id':'str',\n",
    "                                                   'trip_update.timestamp':'Int64',\n",
    "                                                   'trip_update.stop_time_update.stop_sequence':'Int64',\n",
    "                                                   'trip_update.stop_time_update.arrival.delay':'Int64',\n",
    "                                                   'trip_update.stop_time_update.arrival.time':'Int64',\n",
    "                                                   'trip_update.stop_time_update.departure.delay':'Int64',\n",
    "                                                   'trip_update.stop_time_update.departure.time':'Int64',\n",
    "                                                   'trip_update.stop_time_update.stop_id':'str',\n",
    "                                                   'trip_update.stop_time_update.schedule_relationship':'str',\n",
    "                                                   'TUheaderTS':'str',\n",
    "                                                   'trip_update.timestampUTC':'str',\n",
    "                                                   'trip_update.stop_time_update.arrival.timeUTC':'str',\n",
    "                                                   'trip_update.stop_time_update.departure.timeUTC':'str',\n",
    "                                                   'trip_update.trip.start_DateTimeUTC':'str'},\n",
    "                                parse_dates=['trip_update.trip.start_DateTimeUTC'])\n",
    "    return(df_CSV_Raw_TU)\n",
    "\n",
    "#################################################\n",
    "## Remove Redundant, Obsolete, Trivial Records\n",
    "def Df_Remove_ROT(df):\n",
    "    df_NoROT1 = df[\n",
    "        (df['trip_update.trip.schedule_relationship'] != 'CANCELED') \n",
    "        & (df['trip_update.trip.schedule_relationship'] != 'UNSCHEDULED') \n",
    "        & (df['trip_update.stop_time_update.schedule_relationship'] != 'NO_DATA') \n",
    "        & (df['trip_update.stop_time_update.arrival.time'] != 0)\n",
    "    ]\n",
    "    return(df_NoROT1)\n",
    "\n",
    "#################################################\n",
    "## Calculate Scheduled ArrivalTime\n",
    "def Df_SchArrTime(df_NoROT1):\n",
    "    df_NoROT1['Rt.Scheduled_Arrival.Time'] = df_NoROT1['trip_update.stop_time_update.arrival.time'] - df_NoROT1['trip_update.stop_time_update.arrival.delay']\n",
    "    df_NoROT1['Rt.Scheduled_Arrival.TimeUTC'] = pd.DatetimeIndex(\n",
    "        pd.to_datetime(df_NoROT1['Rt.Scheduled_Arrival.Time'],unit='s'),tz='UTC').tz_convert(\n",
    "        'Australia/Sydney').tz_localize(None)\n",
    "    df_NoROT1['Rt.Scheduled_Arrival.TimeUTC'] = df_NoROT1['Rt.Scheduled_Arrival.TimeUTC'].dt.time\n",
    "    df_NoROT1 = df_NoROT1.astype({'Rt.Scheduled_Arrival.TimeUTC':'str'})\n",
    "    df_NoROT1['trip_update.trip.trip_id2'] = df_NoROT1['trip_update.trip.trip_id'].str.rsplit('_', 1).str.get(0)\n",
    "    df_NoROT1['Rt.Scheduled_Arrival.TimeUTC2'] = df_NoROT1['Rt.Scheduled_Arrival.TimeUTC'].str.split(':').apply(lambda x:'%s:%s:%s' % (x[0] if int(x[0])>=4 else int(x[0])+24,x[1],x[2]))\n",
    "    return(df_NoROT1)\n",
    "\n",
    "#################################################\n",
    "## Get Stop Sequence from GTFS Static\n",
    "def Df_GetStaticStopSeq(df_NoROT1):\n",
    "    df_NoROT2 = pd.merge(df_NoROT1,\n",
    "                         df_StTimes[['trip_id','arrival_time','stop_id','stop_sequence']],\n",
    "                         how='left',\n",
    "                         suffixes=('','_ST2'),\n",
    "                         left_on=['trip_update.trip.trip_id2','trip_update.stop_time_update.stop_id','Rt.Scheduled_Arrival.TimeUTC2'],\n",
    "                         right_on=['trip_id','stop_id','arrival_time'])\n",
    "    df_NoROT2['stop_sequence'].fillna(df_NoROT2['trip_update.stop_time_update.stop_sequence'], inplace=True)\n",
    "\n",
    "    ## Fill Empty Stop Sequence Using Existing Data\n",
    "    df_NoROT2 = df_NoROT2.astype({'stop_sequence':'float'})\n",
    "    df_NoROT2['stop_sequence'] = df_NoROT2.groupby(['trip_update.trip.route_id', \n",
    "                                                    'trip_update.trip.trip_id', \n",
    "                                                    'trip_update.trip.start_DateTimeUTC',\n",
    "                                                    'trip_update.stop_time_update.stop_id'\n",
    "                                                   ])['stop_sequence'].apply(lambda x:x.fillna(x.mean()))\n",
    "    df_NoROT2['stop_sequence'] = df_NoROT2['stop_sequence'].round(0).astype('Int64')\n",
    "\n",
    "    ## Drop Columns\n",
    "    df_NoROT2 = df_NoROT2.drop(columns=['trip_id','arrival_time','stop_id'])\n",
    "    return(df_NoROT2)\n",
    "\n",
    "#################################################\n",
    "## Get shape_dist_traveled from GTFS Static\n",
    "def Df_GetStaticDist(df_NoROT2):\n",
    "    df_NoROT3 = pd.merge(df_NoROT2,\n",
    "                         df_StTimes[['trip_id','stop_id','stop_sequence','shape_dist_traveled']],\n",
    "                         how='left',\n",
    "                         suffixes=('','_ST3'),\n",
    "                         left_on=['trip_update.trip.trip_id2','trip_update.stop_time_update.stop_id','stop_sequence'],\n",
    "                         right_on=['trip_id','stop_id','stop_sequence'])\n",
    "    ## Drop Columns\n",
    "    df_NoROT3 = df_NoROT3.drop(columns=['trip_id','stop_id'])\n",
    "    return(df_NoROT3)\n",
    "\n",
    "#################################################\n",
    "## Flag Bad Observations\n",
    "def Df_FlagBad(df_NoROT4):\n",
    "    df_NoROT4.sort_values(by=['trip_update.trip.route_id',\n",
    "                              'trip_update.trip.trip_id',\n",
    "                              'trip_update.trip.start_DateTimeUTC',\n",
    "                              'stop_sequence',\n",
    "                              'trip_update.timestamp'\n",
    "                             ], inplace=True)\n",
    "\n",
    "    df_NoROT4['Bad_Flag0'] = df_NoROT4.groupby(['trip_update.trip.route_id', \n",
    "                                                'trip_update.trip.trip_id', \n",
    "                                                'trip_update.trip.start_DateTimeUTC', \n",
    "                                                'stop_sequence'])['trip_update.timestamp'].diff().ge(240).fillna(0)*1\n",
    "\n",
    "    df_NoROT4['Bad_Flag1'] = df_NoROT4.groupby(['trip_update.trip.route_id', \n",
    "                                                'trip_update.trip.trip_id', \n",
    "                                                'trip_update.trip.start_DateTimeUTC', \n",
    "                                                'stop_sequence'])['Bad_Flag0'].cumsum()\n",
    "    return(df_NoROT4)\n",
    "\n",
    "#################################################\n",
    "## Clean Duplicate Data\n",
    "def Df_Remove_Duplicate(df_Dup):\n",
    "#     df_Dup.sort_values(by=['trip_update.trip.route_id',\n",
    "#                            'trip_update.trip.trip_id',\n",
    "#                            'trip_update.trip.start_DateTimeUTC',\n",
    "#                            'stop_sequence',\n",
    "#                            'trip_update.timestamp'\n",
    "#                           ], inplace=True)\n",
    "    df_Dup2 = df_Dup[df_Dup['Bad_Flag1'] == 0]\n",
    "    ## Drop Columns\n",
    "    df_Dup2 = df_Dup2.drop(columns=['Bad_Flag0','Bad_Flag1'])\n",
    "    df_Unique = df_Dup2.drop_duplicates(subset=['trip_update.trip.route_id',\n",
    "                                                'trip_update.trip.trip_id',\n",
    "                                                'trip_update.trip.start_DateTimeUTC',\n",
    "                                                'stop_sequence'\n",
    "                                               ], keep='last')\n",
    "    return(df_Unique)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-weight:bold; font-size:18px; color:tomato; line-height:1; margin:4px 0px\">\n",
    "    Get Information from GTFS Static\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Static Directory Path\n",
    "FileStaticZip = 'complete_gtfs_scheduled_data_' + FileIdStatic + '.zip'\n",
    "DirStaticZip = DataDir + '/' + FldRawStatic + '/' + FileStaticZip\n",
    "\n",
    "ZipStatic = ZipFile(DirStaticZip)\n",
    "df_StTimes = pd.read_csv(ZipStatic.open('stop_times.txt'),\n",
    "                         dtype={'trip_id':'str','arrival_time':'str','departure_time':'str','stop_id':'str',\n",
    "                                'stop_sequence':'Int64','stop_headsign':'str','pickup_type':'int','drop_off_type':'int',\n",
    "                                'shape_dist_traveled':'float','timepoint':'int','stop_note':'str'},\n",
    "                        )\n",
    "# df_StTimes.head(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-weight:bold; font-size:18px; color:tomato; line-height:1; margin:4px 0px\">\n",
    "    Filter A List of Routes in Raw CSV Files and Combine All Filtered Records \n",
    "    <br>\n",
    "    (Based on TU trip_update.trip.route_id)\n",
    "</p>\n",
    "Notes:\n",
    "<ul style=\"line-height:1.4; margin:0px 0px\">\n",
    "  <li>Only SCHEDULED and ADDED trips in 'trip_update.trip.schedule_relationship' are included.</li>\n",
    "  <li>UNSCHEDULED and CANCELED trips are not included.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# # ## FOR A LIST OF ROUTES\n",
    "# # Rt_Route = FN_Rt_RouteList.split('.')[0]\n",
    "# # Flt_Route_Rt = loadtxt(File_Rt_RouteList, comments=\"#\", dtype='str')\n",
    "\n",
    "# # ## FOR LIST OF ROUTES BY AGENCY\n",
    "# # Rt_Route = Flt_Agency\n",
    "# # Flt_Route_Rt = List_AgencyRtRoutes\n",
    "\n",
    "# ## Record Start Time\n",
    "# tStart = datetime.now()\n",
    "# print('PROCESSING DATA FOR', FileTP, \"...\")\n",
    "# print('Time Start:', tStart.isoformat(' ', 'seconds'))\n",
    "       \n",
    "# ## Define File Path\n",
    "# PathTransTUrtNoROT = DirTransTU + '/' + GTFS_TU_Prefix + '_' + FileTP + '_rt' + Routes + '_NoROT.csv'\n",
    "# PathTransTUrtCln = DirClnTU + '/' + GTFS_TU_Prefix + '_' + FileTP + '_rt' + Routes + '_Cln.csv'\n",
    "\n",
    "# ## Check if file exists. Remove if exist.\n",
    "# if os.path.exists(PathTransTUrtNoROT):\n",
    "#     os.remove(PathTransTUrtNoROT)\n",
    "# if os.path.exists(PathTransTUrtCln):\n",
    "#     os.remove(PathTransTUrtCln)\n",
    "\n",
    "# ## Filter Route and Concatenate All CSV Files in Folder (add new column with Filename as trace)\n",
    "# all_files = glob.glob(os.path.join(DirRawCSVtu, GTFS_TU_Prefix + '*.csv'))\n",
    "\n",
    "# iFile = 0\n",
    "# df_Con = []\n",
    "\n",
    "# for f in all_files:\n",
    "\n",
    "#     ## Count File\n",
    "#     iFile = iFile + 1\n",
    "\n",
    "#     ## Get FullFileName from Path\n",
    "#     FullFileName = f.split('\\\\')[-1]\n",
    "#     ## FileName exclude Extension\n",
    "#     FNexExt = os.path.splitext(FullFileName)[0]\n",
    "\n",
    "#     if iFile == 1:\n",
    "#         ## Call function to read raw TU CSV files\n",
    "#         df_Con = Read_CSV_Raw_TU(f)\n",
    "#         ## Grab the records associated with the Flt_Route\n",
    "#         df_Con_Flt = df_Con[df_Con['trip_update.trip.route_id'].isin(Flt_Routes_Rt)]\n",
    "#         ## Call function remove ROT records\n",
    "#         df_Con_Flt_NoROT1 = Df_Remove_ROT(df_Con_Flt)\n",
    "#     else:\n",
    "#         ## Call function to read raw TU CSV files\n",
    "#         df_X = Read_CSV_Raw_TU(f)\n",
    "#         ## Grab the records associated with the Flt_Route\n",
    "#         df_X_Flt = df_X[df_X['trip_update.trip.route_id'].isin(Flt_Routes_Rt)]\n",
    "#         ## Call function remove ROT records\n",
    "#         df_X_Flt_NoROT1 = Df_Remove_ROT(df_X_Flt)\n",
    "\n",
    "#         ## Combine records from df_Con_Flt and df_X_Flt\n",
    "#         df_Con_Flt_NoROT1 = pd.concat([df_Con_Flt_NoROT1, df_X_Flt_NoROT1], ignore_index=True)\n",
    "\n",
    "# ## Calculate Scheduled ArrivalTime\n",
    "# df_Con_Flt_NoROT1 = Df_SchArrTime(df_Con_Flt_NoROT1)\n",
    "\n",
    "# ## Get Stop Sequence from GTFS Static\n",
    "# df_Con_Flt_NoROT2 = Df_GetStaticStopSeq(df_Con_Flt_NoROT1)\n",
    "\n",
    "# ## Get shape_dist_traveled from GTFS Static\n",
    "# df_Con_Flt_NoROT3 = Df_GetStaticDist(df_Con_Flt_NoROT2)\n",
    "\n",
    "# ## Export concatenated files to CSV\n",
    "# df_Con_Flt_NoROT = df_Con_Flt_NoROT3\n",
    "# df_Con_Flt_NoROT.to_csv(PathTransTUrtNoROT, index=False)\n",
    "\n",
    "# ## Clean Duplicate Data\n",
    "# df_ConTU_Flt_Cln = Df_Remove_Duplicate(df_Con_Flt_NoROT)\n",
    "\n",
    "# ## Export Cleaned Data to CSV\n",
    "# df_ConTU_Flt_Cln.to_csv(PathTransTUrtCln, index=False)\n",
    "\n",
    "# ## Record End Time\n",
    "# tEnd = datetime.now()\n",
    "# print()\n",
    "# print('Routes', Routes, '-->', iFile, 'Files Processed:', tEnd.isoformat(' ', 'seconds') + '; Time Spent:', tEnd-tStart)\n",
    "# print('After ROT Removed:', df_Con_Flt_NoROT.shape)\n",
    "# print('Cleaned:', df_ConTU_Flt_Cln.shape)\n",
    "# # print('...')\n",
    "\n",
    "# print('')\n",
    "# print('COMPLETED ON', datetime.now())\n",
    "# # print('Total Time Spent:', tEnd-tStart)\n",
    "# print('Transformed files saved in:', DirTransTU)\n",
    "# print('Cleaned files saved in:', DirClnTU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-weight:bold; font-size:18px; color:tomato; line-height:1; margin:4px 0px\">\n",
    "    Filter All Routes by Agency in Raw CSV Files, Combine All Filtered Records, Get Unique Records \n",
    "    <br>\n",
    "    (Based on TU trip_update.trip.route_id)\n",
    "</p>\n",
    "Notes:\n",
    "<ul style=\"line-height:1.4; margin:0px 0px\">\n",
    "  <li>Only SCHEDULED and ADDED trips in 'trip_update.trip.schedule_relationship' are included.</li>\n",
    "  <li>UNSCHEDULED and CANCELED trips are not included.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ## FOR LIST OF ROUTES BY AGENCY\n",
    "# Rt_Route = Flt_Agency\n",
    "# Flt_Route_Rt = List_AgencyRtRoutes\n",
    "\n",
    "# ## Record Start Time\n",
    "# tStart = datetime.now()\n",
    "# print('PROCESSING DATA FOR', FileTP, \"...\")\n",
    "# print('Time Start:', tStart.isoformat(' ', 'seconds'))\n",
    "       \n",
    "# ## Define File Path\n",
    "# PathTransTUrtNoROT = DirTransTU + '/' + GTFS_TU_Prefix + '_' + FileTP + '_rt' + Rt_Route + '_NoROT.csv'\n",
    "# PathTransTUrtCln = DirClnTU + '/' + GTFS_TU_Prefix + '_' + FileTP + '_rt' + Rt_Route + '_Cln.csv'\n",
    "\n",
    "# ## Check if file exists. Remove if exist.\n",
    "# if os.path.exists(PathTransTUrtNoROT):\n",
    "#     os.remove(PathTransTUrtNoROT)\n",
    "# if os.path.exists(PathTransTUrtCln):\n",
    "#     os.remove(PathTransTUrtCln)\n",
    "\n",
    "# ## Filter Route and Concatenate All CSV Files in Folder (add new column with Filename as trace)\n",
    "# all_files = glob.glob(os.path.join(DirRawCSVtu, GTFS_TU_Prefix + '*.csv'))\n",
    "\n",
    "# iFile = 0\n",
    "# df_Con = []\n",
    "\n",
    "# for f in all_files:\n",
    "\n",
    "#     ## Count File\n",
    "#     iFile = iFile + 1\n",
    "\n",
    "#     ## Get FullFileName from Path\n",
    "#     FullFileName = f.split('\\\\')[-1]\n",
    "#     ## FileName exclude Extension\n",
    "#     FNexExt = os.path.splitext(FullFileName)[0]\n",
    "\n",
    "#     if iFile == 1:\n",
    "#         ## Call function to read raw TU CSV files\n",
    "#         df_Con = Read_CSV_Raw_TU(f)\n",
    "#         ## Grab the records associated with the Flt_Route\n",
    "#         df_Con_Flt = df_Con[df_Con['trip_update.trip.route_id'].isin(Flt_Route_Rt)]\n",
    "#         ## Call function remove ROT records\n",
    "#         df_Con_Flt_NoROT1 = Df_Remove_ROT(df_Con_Flt)\n",
    "#     else:\n",
    "#         ## Call function to read raw TU CSV files\n",
    "#         df_X = Read_CSV_Raw_TU(f)\n",
    "#         ## Grab the records associated with the Flt_Route\n",
    "#         df_X_Flt = df_X[df_X['trip_update.trip.route_id'].isin(Flt_Route_Rt)]\n",
    "#         ## Call function remove ROT records\n",
    "#         df_X_Flt_NoROT1 = Df_Remove_ROT(df_X_Flt)\n",
    "\n",
    "#         ## Combine records from df_Con_Flt and df_X_Flt\n",
    "#         df_Con_Flt_NoROT1 = pd.concat([df_Con_Flt_NoROT1, df_X_Flt_NoROT1], ignore_index=True)\n",
    "\n",
    "# ## Calculate Scheduled ArrivalTime\n",
    "# df_Con_Flt_NoROT1 = Df_SchArrTime(df_Con_Flt_NoROT1)\n",
    "\n",
    "# ## Get Stop Sequence from GTFS Static\n",
    "# df_Con_Flt_NoROT2 = Df_GetStaticStopSeq(df_Con_Flt_NoROT1)\n",
    "\n",
    "# ## Get shape_dist_traveled from GTFS Static\n",
    "# df_Con_Flt_NoROT3 = Df_GetStaticDist(df_Con_Flt_NoROT2)\n",
    "\n",
    "# ## Export concatenated files to CSV\n",
    "# df_Con_Flt_NoROT = df_Con_Flt_NoROT3\n",
    "# df_Con_Flt_NoROT.to_csv(PathTransTUrtNoROT, index=False)\n",
    "\n",
    "# ## Clean Duplicate Data\n",
    "# df_ConTU_Flt_Cln = Df_Remove_Duplicate(df_Con_Flt_NoROT)\n",
    "\n",
    "# ## Export Cleaned Data to CSV\n",
    "# df_ConTU_Flt_Cln.to_csv(PathTransTUrtCln, index=False)\n",
    "\n",
    "# ## Record End Time\n",
    "# tEnd = datetime.now()\n",
    "# print()\n",
    "# print('Route', Rt_Route, '-->', iFile, 'Files Processed:', tEnd.isoformat(' ', 'seconds') + '; Time Spent:', tEnd-tStart)\n",
    "# print('After ROT Removed:', df_Con_Flt_NoROT.shape)\n",
    "# print('Cleaned:', df_ConTU_Flt_Cln.shape)\n",
    "# # print('...')\n",
    "\n",
    "# print('')\n",
    "# print('COMPLETED ON', datetime.now())\n",
    "# # print('Total Time Spent:', tEnd-tStart)\n",
    "# print('Transformed files saved in:', DirTransTU)\n",
    "# print('Cleaned files saved in:', DirClnTU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-weight:bold; font-size:18px; color:red; line-height:1; margin:4px 0px\">\n",
    "    TO BE UPDATED:\n",
    "    <br>\n",
    "    Filter Individual Routes in Raw CSV Files and Combine All Filtered Records \n",
    "    <br>\n",
    "    (Based on TU trip_update.trip.route_id)\n",
    "</p>\n",
    "Notes:\n",
    "<ul style=\"line-height:1.4; margin:0px 0px\">\n",
    "  <li>Only SCHEDULED and ADDED trips in 'trip_update.trip.schedule_relationship' are included.</li>\n",
    "  <li>UNSCHEDULED and CANCELED trips are not included.</li>\n",
    "</ul>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ###############\n",
    "# ## OPTIONALS ##\n",
    "# ## Check Realtime Route List\n",
    "# with open(File_Rt_RouteList) as Rt_RouteList:\n",
    "#     for Rt_R in Rt_RouteList:\n",
    "#         Rt_Route = Rt_R.split('#', 1)[0].strip()\n",
    "#         if not Rt_Route:\n",
    "#             continue\n",
    "#         Flt_Route_Rt = [Rt_Route]\n",
    "#         print(Flt_Route_Rt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# ## Record Start Time\n",
    "# tStart = datetime.now()\n",
    "# print('PROCESSING DATA FOR', FileTP, \"...\")\n",
    "# print('Time Start:', tStart.isoformat(' ', 'seconds'))\n",
    "\n",
    "# with open(File_Rt_RouteList) as Rt_RouteList:\n",
    "#     for Rt_R in Rt_RouteList:\n",
    "#         Rt_Route = Rt_R.split('#', 1)[0].strip()\n",
    "#         if not Rt_Route:\n",
    "#             continue\n",
    "#         Flt_Route_Rt = [Rt_Route]\n",
    "\n",
    "#         ## Record Start Time for Route\n",
    "#         tStartR = datetime.now()\n",
    "        \n",
    "#         ## Define File Path\n",
    "#         PathTransTUrtNoROT = DirTransTU + '/' + GTFS_TU_Prefix + '_' + FileTP + '_rt' + Rt_Route + '_NoROT.csv'\n",
    "#         PathTransTUrtCln = DirClnTU + '/' + GTFS_TU_Prefix + '_' + FileTP + '_rt' + Rt_Route + '_Cln.csv'\n",
    "\n",
    "#         ## Check if file exists. Remove if exist.\n",
    "#         if os.path.exists(PathTransTUrtNoROT):\n",
    "#             os.remove(PathTransTUrtNoROT)\n",
    "#         if os.path.exists(PathTransTUrtCln):\n",
    "#             os.remove(PathTransTUrtCln)\n",
    "\n",
    "#         ## Filter Route and Concatenate All CSV Files in Folder (add new column with Filename as trace)\n",
    "#         all_files = glob.glob(os.path.join(DirRawCSVtu, GTFS_TU_Prefix + '*.csv'))\n",
    "\n",
    "#         iFile = 0\n",
    "#         df_Con = []\n",
    "\n",
    "#         for f in all_files:\n",
    "\n",
    "#             ## Count File\n",
    "#             iFile = iFile + 1\n",
    "\n",
    "#             ## Get FullFileName from Path\n",
    "#             FullFileName = f.split('\\\\')[-1]\n",
    "#             ## FileName exclude Extension\n",
    "#             FNexExt = os.path.splitext(FullFileName)[0]\n",
    "\n",
    "#             if iFile == 1:\n",
    "#                 ## Call function to read raw TU CSV files\n",
    "#                 df_Con = Read_CSV_Raw_TU(f)\n",
    "#                 ## Grab the records associated with the Flt_Route\n",
    "#                 df_Con_Flt = df_Con[df_Con['trip_update.trip.route_id'].isin(Flt_Route_Rt)]\n",
    "#                 ## Call function remove ROT records\n",
    "#                 df_Con_Flt_NoROT1 = Df_Remove_ROT(df_Con_Flt)\n",
    "#             else:\n",
    "#                 ## Call function to read raw TU CSV files\n",
    "#                 df_X = Read_CSV_Raw_TU(f)\n",
    "#                 ## Grab the records associated with the Flt_Route\n",
    "#                 df_X_Flt = df_X[df_X['trip_update.trip.route_id'].isin(Flt_Route_Rt)]\n",
    "#                 ## Call function remove ROT records\n",
    "#                 df_X_Flt_NoROT1 = Df_Remove_ROT(df_X_Flt)\n",
    "\n",
    "#                 ## Combine records from df_Con_Flt and df_X_Flt\n",
    "#                 df_Con_Flt_NoROT1 = pd.concat([df_Con_Flt_NoROT1, df_X_Flt_NoROT1], ignore_index=True)\n",
    "\n",
    "#         ## Calculate Scheduled ArrivalTime\n",
    "#         df_Con_Flt_NoROT1 = Df_SchArrTime(df_Con_Flt_NoROT1)\n",
    "\n",
    "#         ## Get Stop Sequence from GTFS Static\n",
    "#         df_Con_Flt_NoROT2 = Df_GetStaticStopSeq(df_Con_Flt_NoROT1)\n",
    "\n",
    "#         ## Get shape_dist_traveled from GTFS Static\n",
    "#         df_Con_Flt_NoROT3 = Df_GetStaticDist(df_Con_Flt_NoROT2)\n",
    "\n",
    "#         ## Export concatenated files to CSV\n",
    "#         df_Con_Flt_NoROT = df_Con_Flt_NoROT3\n",
    "#         df_Con_Flt_NoROT.to_csv(PathTransTUrtNoROT, index=False)\n",
    "\n",
    "#         ## Clean Duplicate Data\n",
    "#         df_ConTU_Flt_Cln = Df_Remove_Duplicate(df_Con_Flt_NoROT)\n",
    "\n",
    "#         ## Export Cleaned Data to CSV\n",
    "#         df_ConTU_Flt_Cln.to_csv(PathTransTUrtCln, index=False)\n",
    "\n",
    "\n",
    "#         ## Record End Time\n",
    "#         tEnd = datetime.now()\n",
    "#         print()\n",
    "#         print('Route', Rt_Route, '-->', iFile, 'Files Processed:', tEnd.isoformat(' ', 'seconds') + '; Time Spent:', tEnd-tStartR)\n",
    "#         print('After ROT Removed:', df_Con_Flt_NoROT.shape)\n",
    "#         print('Cleaned:', df_ConTU_Flt_Cln.shape)\n",
    "#         print('...')\n",
    "\n",
    "# print('')\n",
    "# print('COMPLETED ON', datetime.now())\n",
    "# print('Total Time Spent:', tEnd-tStart)\n",
    "# print('Transformed files saved in:', DirTransTU)\n",
    "# print('Cleaned files saved in:', DirClnTU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-weight:bold; font-size:18px; color:tomato; line-height:1; margin:4px 0px\">\n",
    "    FOR ARTEMIS: Combine Complete Raw CSV Files\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Record Start Time\n",
    "tStart = datetime.now()\n",
    "print('PROCESSING DATA FOR', FileTP, \"...\")\n",
    "print('Time Start:', tStart.isoformat(' ', 'seconds'))\n",
    "       \n",
    "## Define File Path\n",
    "PathTransTUrtNoROT = DirTransTU + '/' + GTFS_TU_Prefix + '_' + FileTP + '_NoROT.csv'\n",
    "PathTransTUrtCln = DirClnTU + '/' + GTFS_TU_Prefix + '_' + FileTP + '_Cln.csv'\n",
    "\n",
    "## Check if file exists. Remove if exist.\n",
    "if os.path.exists(PathTransTUrtNoROT):\n",
    "    os.remove(PathTransTUrtNoROT)\n",
    "if os.path.exists(PathTransTUrtCln):\n",
    "    os.remove(PathTransTUrtCln)\n",
    "\n",
    "## Filter Route and Concatenate All CSV Files in Folder (add new column with Filename as trace)\n",
    "all_files = glob.glob(os.path.join(DirRawCSVtu, GTFS_TU_Prefix + '*.csv'))\n",
    "\n",
    "iFile = 0\n",
    "df_Con = []\n",
    "\n",
    "for f in all_files:\n",
    "\n",
    "    ## Count File\n",
    "    iFile = iFile + 1\n",
    "\n",
    "    ## Get FullFileName from Path\n",
    "##    FullFileName = f.split('/')[-1]     ## FOR LINUX COMPUTERS\n",
    "    FullFileName = f.split('\\\\')[-1]    ## FOR WINDOWS COMPUTERS\n",
    "    ## FileName exclude Extension\n",
    "    FNexExt = os.path.splitext(FullFileName)[0]\n",
    "\n",
    "    if iFile == 1:\n",
    "        ## Call function to read raw TU CSV files\n",
    "        df_Con = Read_CSV_Raw_TU(f)\n",
    "        ## Call function remove ROT records\n",
    "        df_Con_NoROT1 = Df_Remove_ROT(df_Con)\n",
    "    else:\n",
    "        ## Call function to read raw TU CSV files\n",
    "        df_X = Read_CSV_Raw_TU(f)\n",
    "        ## Call function remove ROT records\n",
    "        df_X_NoROT1 = Df_Remove_ROT(df_X)\n",
    "\n",
    "        ## Combine records from df_Con_Flt and df_X_Flt\n",
    "        df_Con_NoROT1 = pd.concat([df_Con_NoROT1, df_X_NoROT1], ignore_index=True)\n",
    "\n",
    "## Calculate Scheduled ArrivalTime\n",
    "df_Con_NoROT1 = Df_SchArrTime(df_Con_NoROT1)\n",
    "\n",
    "## Get Stop Sequence from GTFS Static\n",
    "df_Con_NoROT2 = Df_GetStaticStopSeq(df_Con_NoROT1)\n",
    "\n",
    "## Get shape_dist_traveled from GTFS Static\n",
    "df_Con_NoROT3 = Df_GetStaticDist(df_Con_NoROT2)\n",
    "\n",
    "## Flag Bad Observations\n",
    "df_Con_NoROT4 = Df_FlagBad(df_Con_NoROT3)\n",
    "\n",
    "## Export concatenated files to CSV\n",
    "df_Con_NoROT = df_Con_NoROT4\n",
    "df_Con_NoROT.to_csv(PathTransTUrtNoROT, index=False)\n",
    "\n",
    "## Clean Duplicate Data\n",
    "df_ConTU_Cln = Df_Remove_Duplicate(df_Con_NoROT)\n",
    "## Export Cleaned Data to CSV\n",
    "df_ConTU_Cln.to_csv(PathTransTUrtCln, index=False)\n",
    "\n",
    "## Record End Time\n",
    "tEnd = datetime.now()\n",
    "print(iFile, 'Files Processed:', tEnd.isoformat(' ', 'seconds') + '; Time Spent:', tEnd-tStart)\n",
    "print('After ROT Removed:', df_Con_NoROT.shape)\n",
    "print('Cleaned:', df_ConTU_Cln.shape)\n",
    "print('COMPLETED ON', datetime.now())\n",
    "print('Transformed file saved in:', PathTransTUrtNoROT)\n",
    "print('Cleaned file saved in:', PathTransTUrtCln)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-weight:bold; font-size:18px; color:yellow; line-height:normal; margin:0px; padding:4px; background-color:gray\">\n",
    "    Ad Hoc: Define Functions for Ad Hoc Tasks\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#################################################\n",
    "## Read NotROT TU CSV\n",
    "def Read_CSV_NotROT_TU(f):\n",
    "    df_CSV_NotROT_TU = pd.read_csv(f, sep=',', dtype={'id':'str',\n",
    "                                                      'trip_update.trip.trip_id':'str',\n",
    "                                                      'trip_update.trip.start_time':'str',\n",
    "                                                      'trip_update.trip.start_date':'str',\n",
    "                                                      'trip_update.trip.schedule_relationship':'str',\n",
    "                                                      'trip_update.trip.route_id':'str',\n",
    "                                                      'trip_update.vehicle.id':'str',\n",
    "                                                      'trip_update.timestamp':'Int64',\n",
    "                                                      'trip_update.stop_time_update.stop_sequence':'Int64',\n",
    "                                                      'trip_update.stop_time_update.arrival.delay':'Int64',\n",
    "                                                      'trip_update.stop_time_update.arrival.time':'Int64',\n",
    "                                                      'trip_update.stop_time_update.departure.delay':'Int64',\n",
    "                                                      'trip_update.stop_time_update.departure.time':'Int64',\n",
    "                                                      'trip_update.stop_time_update.stop_id':'str',\n",
    "                                                      'trip_update.stop_time_update.schedule_relationship':'str',\n",
    "                                                      'TUheaderTS':'str',\n",
    "                                                      'trip_update.timestampUTC':'str',\n",
    "                                                      'trip_update.stop_time_update.arrival.timeUTC':'str',\n",
    "                                                      'trip_update.stop_time_update.departure.timeUTC':'str',\n",
    "                                                      'trip_update.trip.start_DateTimeUTC':'str',\n",
    "                                                      'Rt.Scheduled_Arrival.Time':'str',\n",
    "                                                      'Rt.Scheduled_Arrival.TimeUTC':'str',\n",
    "                                                      'trip_update.trip.trip_id2':'str',\n",
    "                                                      'Rt.Scheduled_Arrival.TimeUTC2':'str',\n",
    "                                                      'stop_sequence':'Int64',\n",
    "                                                      'shape_dist_traveled':'float',\n",
    "                                                      'Bad_Flag0':'Int32',\n",
    "                                                      'Bad_Flag1':'Int32'\n",
    "                                                     }, \n",
    "                                   parse_dates=['trip_update.stop_time_update.arrival.timeUTC','trip_update.trip.start_DateTimeUTC']\n",
    "                                  )\n",
    "    return(df_CSV_NotROT_TU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "############################################################################\n",
    "## Fill Empty Stop Sequence Using Existing Data AND Flag Bad Observations ##\n",
    "def Fill_Empty_StopSeq2(df, df_S):\n",
    "    df = df.astype({'stop_sequence':'float'})\n",
    "    df['stop_sequence'] = df.groupby(['trip_update.trip.route_id', \n",
    "                                      'trip_update.trip.trip_id', \n",
    "                                      'trip_update.trip.start_DateTimeUTC',\n",
    "                                      'trip_update.stop_time_update.stop_id'\n",
    "                                     ])['stop_sequence'].apply(lambda x:x.fillna(x.mean()))\n",
    "    df['stop_sequence'] = df['stop_sequence'].round(0).astype('Int64')\n",
    "\n",
    "    ## Get shape_dist_traveled from GTFS Static\n",
    "    df1 = pd.merge(df, \n",
    "                   df_S[['trip_id','stop_id','stop_sequence','shape_dist_traveled']],\n",
    "                   how='left',\n",
    "                   suffixes=('','_S'),\n",
    "                   left_on=['trip_update.trip.trip_id2','trip_update.stop_time_update.stop_id','stop_sequence'],\n",
    "                   right_on=['trip_id','stop_id','stop_sequence'])\n",
    "    df1['shape_dist_traveled'].fillna(df1['shape_dist_traveled_S'], inplace=True)\n",
    "    ## Drop Columns\n",
    "    df2 = df1.drop(columns=['Bad_Flag0','Bad_Flag1','trip_id','stop_id','shape_dist_traveled_S'])\n",
    "    ## Flag Bad Observations\n",
    "    df2.sort_values(by=['trip_update.trip.route_id', \n",
    "                        'trip_update.trip.trip_id',\n",
    "                        'trip_update.trip.start_DateTimeUTC',\n",
    "                        'stop_sequence',\n",
    "                        'trip_update.timestamp'\n",
    "                       ], inplace=True)\n",
    "\n",
    "    ## Bad when 300s gap (from TP to TP)\n",
    "    df2['Bad_Flag0'] = df2.groupby(['trip_update.trip.route_id', \n",
    "                                    'trip_update.trip.trip_id', \n",
    "                                    'trip_update.trip.start_DateTimeUTC', \n",
    "                                    'stop_sequence'])['trip_update.timestamp'].diff().ge(300).fillna(0)*1\n",
    "\n",
    "    df2['Bad_Flag1'] = df2.groupby(['trip_update.trip.route_id', \n",
    "                                    'trip_update.trip.trip_id', \n",
    "                                    'trip_update.trip.start_DateTimeUTC', \n",
    "                                    'stop_sequence'])['Bad_Flag0'].cumsum()\n",
    "\n",
    "    return(df2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-weight:bold; font-size:18px; color:yellow; line-height:normal; margin:0px; padding:4px; background-color:gray\">\n",
    "    Ad Hoc: From NotROT to Cleaned Unique\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "tStart1 = datetime.now()\n",
    "print('Main Time Start:', tStart1.isoformat(' ', 'seconds'))\n",
    "print('')\n",
    "\n",
    "for iDay in range(DayInMonth, DayInMonth+1):\n",
    "\n",
    "    FileId = FileTP + 'd' + (str(iDay).zfill(2))\n",
    "\n",
    "    df_Con = []\n",
    "    for iTP in range(1, 7):\n",
    "\n",
    "        ## Record Start Time\n",
    "        tStart = datetime.now()\n",
    "        print('PROCESSING DATA FOR', FileId + 'TP' + str(iTP), \"...\")\n",
    "        print('Time Start:', tStart.isoformat(' ', 'seconds'))\n",
    "\n",
    "        ## Define Input File Path\n",
    "        PathTransTUrtNoROT = DirTransTU + '/' + GTFS_TU_Prefix + '_' + FileId + 'tp' + str(iTP) + '_NoROT.csv'\n",
    "\n",
    "        ## Call function to read NotROT TU CSV files\n",
    "        df_Con_NoROT = Read_CSV_NotROT_TU(PathTransTUrtNoROT)\n",
    "        print(FileId + 'TP' + str(iTP), df_Con_NoROT.shape)\n",
    "\n",
    "        ## Define Output File Path\n",
    "        PathTransTUrtCln = DirClnTU + '/' + GTFS_TU_Prefix + '_' + FileId + 'tp' + str(iTP) + '_Cln.csv'\n",
    "        \n",
    "        ## Clean Duplicate Data\n",
    "        df_ConTU_Cln = Df_Remove_Duplicate(df_Con_NoROT)\n",
    "        ## Export Cleaned Data to CSV\n",
    "        df_ConTU_Cln.to_csv(PathTransTUrtCln, index=False)\n",
    "\n",
    "        ## Record End Time\n",
    "        tEnd = datetime.now()\n",
    "        print('After ROT Removed:', df_Con_NoROT.shape)\n",
    "        print('Cleaned:', df_ConTU_Cln.shape)\n",
    "        print('Time Spent:', tEnd-tStart)\n",
    "        print('')\n",
    "\n",
    "tEnd1 = datetime.now()\n",
    "print('Total Time Spent:', tEnd1-tStart1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-weight:bold; font-size:18px; color:yellow; line-height:normal; margin:0px; padding:4px; background-color:gray\">\n",
    "    Ad Hoc: Get Information from GTFS Static (Route List by Agency and Filter Route Type 700)\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#############################################\n",
    "## Static Directory Path\n",
    "FileStaticZip = 'complete_gtfs_scheduled_data_' + FileIdStatic + '.zip'\n",
    "DirStaticZip = DataDir + '/' + FldRawStatic + '/' + FileStaticZip\n",
    "\n",
    "ZipStatic = ZipFile(DirStaticZip)\n",
    "df_StTimes = pd.read_csv(ZipStatic.open('stop_times.txt'),\n",
    "                         dtype={'trip_id':'str','arrival_time':'str','departure_time':'str','stop_id':'str',\n",
    "                                'stop_sequence':'Int64','stop_headsign':'str','pickup_type':'int','drop_off_type':'int',\n",
    "                                'shape_dist_traveled':'float','timepoint':'int','stop_note':'str'},\n",
    "                        )\n",
    "# df_StTimes.head(2)\n",
    "\n",
    "#############################################\n",
    "## Get List of Routes based on Agency Name ##\n",
    "df_Agency = pd.read_csv(ZipStatic.open('agency.txt'),dtype='unicode')\n",
    "df_Routes = pd.read_csv(ZipStatic.open('routes.txt'),dtype='unicode')\n",
    "df_Routes['RT_route_id'] = df_Routes['agency_id'] + '_' + df_Routes['route_short_name']\n",
    "\n",
    "df_RoutesAgency = pd.merge(df_Routes,\n",
    "                           df_Agency[['agency_id','agency_name']],\n",
    "                           how='left',\n",
    "                           suffixes=('','_Ag'),\n",
    "                           on=['agency_id'])\n",
    "\n",
    "df_RoutesAgency_Flt = df_RoutesAgency[df_RoutesAgency['agency_name'].isin([Flt_Agency])]\n",
    "# df_RoutesAgency_Flt.head(1)\n",
    "\n",
    "List_AgencyRtRoutes = df_RoutesAgency_Flt['RT_route_id']\n",
    "# List_AgencyRtRoutes\n",
    "\n",
    "df_RoutesAgency_FltRT = df_RoutesAgency_Flt[df_RoutesAgency_Flt['route_type'] == '700']\n",
    "# df_RoutesAgency_FltRT.head(1)\n",
    "\n",
    "List_AgencyRtRoutes_700 = df_RoutesAgency_FltRT['RT_route_id']\n",
    "# List_AgencyRtRoutes_700"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-weight:bold; font-size:18px; color:yellow; line-height:normal; margin:0px; padding:4px; background-color:gray\">\n",
    "    Ad Hoc: Combine NotROT and Cleaned: Daily by Agency and Route Type\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Record Start Time\n",
    "tStart = datetime.now()\n",
    "print('PROCESSING DATA FOR', FileTP, \"...\")\n",
    "print('Time Start:', tStart.isoformat(' ', 'seconds'))\n",
    "print('')\n",
    "\n",
    "for iDay in range(DayInMonth, DayInMonth+1):\n",
    "\n",
    "    FileId = FileTP + 'd' + (str(iDay).zfill(2))\n",
    "\n",
    "    df_Con = []\n",
    "    for iTP in range(1, 7):\n",
    "\n",
    "        ## Define Input File Path\n",
    "        PathTransTUrtNoROT = DirTransTU + '/' + GTFS_TU_Prefix + '_' + FileId + 'tp' + str(iTP) + '_NoROT.csv'\n",
    "\n",
    "        if iTP == 1:\n",
    "            ## Call function to read NotROT TU CSV files\n",
    "            df_Con = Read_CSV_NotROT_TU(PathTransTUrtNoROT)\n",
    "            print(FileId + 'TP' + str(iTP), df_Con.shape)\n",
    "        else:\n",
    "            ## Call function to read NotROT TU CSV files\n",
    "            df_X = Read_CSV_NotROT_TU(PathTransTUrtNoROT)\n",
    "            print(FileId + 'TP' + str(iTP), df_X.shape)\n",
    "\n",
    "            ## Combine records from df_Con_Flt and df_X_Flt\n",
    "            df_Con = pd.concat([df_Con, df_X], ignore_index=True)\n",
    "\n",
    "    ############################################\n",
    "    ## Daily NotROT Dataset by Agency ##\n",
    "\n",
    "    ## FOR LIST OF ROUTES BY AGENCY\n",
    "    Agency = Flt_Agency\n",
    "    Flt_Routes_Rt = List_AgencyRtRoutes_700\n",
    "\n",
    "    ## Define Output File Path by Agency\n",
    "    PathTransTUrtNoROT_Agency = DirTransTU + '/' + GTFS_TU_Prefix + '_' + FileId + \"_\" + Agency + '_700_NoROT.csv'\n",
    "        \n",
    "    ## Check if file exists. Remove if exist.\n",
    "    if os.path.exists(PathTransTUrtNoROT_Agency):\n",
    "        os.remove(PathTransTUrtNoROT_Agency)\n",
    "\n",
    "    ## Grab the records associated with the Agency\n",
    "    df_Con_Flt = df_Con[df_Con['trip_update.trip.route_id'].isin(Flt_Routes_Rt)]\n",
    "\n",
    "    ## Fill Empty Stop Sequence Using Existing Data ##\n",
    "    df_Con_Flt = Fill_Empty_StopSeq2(df_Con_Flt, df_StTimes)\n",
    "\n",
    "    ## Export Agency Daily NotROT Data to CSV\n",
    "    df_Con_Flt.to_csv(PathTransTUrtNoROT_Agency, index=False)\n",
    "\n",
    "    print('Combined Dataset:', FileId, df_Con.shape)\n",
    "    print('Combined Dataset by Agency:', Agency, FileId, df_Con_Flt.shape)\n",
    "    print('')\n",
    "    \n",
    "\n",
    "## Record End Time\n",
    "tEnd = datetime.now()\n",
    "print(iDay, 'Days Processed:', tEnd.isoformat(' ', 'seconds') + '; Time Spent:', tEnd-tStart)\n",
    "print('COMPLETED ON', datetime.now())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.float_format = '{:.4f}'.format\n",
    "df_Con_Flt.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.reset_option('^display.', silent=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-weight:bold; font-size:18px; color:yellow; line-height:normal; margin:0px; padding:4px; background-color:gray\">\n",
    "    Ad Hoc: Extract Data by TripID from Combined Daily by Agency and Route Type\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FileId = '2020m06d04'\n",
    "Agency = Flt_Agency\n",
    "# PathTransTUrtNoROT_Agency = DirTransTU + '/' + GTFS_TU_Prefix + '_' + FileId + \"_\" + Agency + '_NoROT.csv'\n",
    "# df_Con_Flt = pd.read_csv(PathTransTUrtNoROT_Agency, sep=',',dtype='unicode')\n",
    "df_Con_Flt.sort_values(by=['trip_update.trip.route_id',\n",
    "                           'trip_update.trip.trip_id',\n",
    "                           'trip_update.trip.start_DateTimeUTC',\n",
    "                           'stop_sequence',\n",
    "                           'trip_update.timestamp'\n",
    "                          ], inplace=True)\n",
    "# df_Con_Flt.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "YYYYMMDD = '20200604'\n",
    "TripID = '746158'\n",
    "\n",
    "df_Con_flt1 = df_Con_Flt.loc[df_Con_Flt['trip_update.trip.start_date'] == YYYYMMDD]\n",
    "# df_Con_flt1.to_csv(Out, index=False)\n",
    "print('Filtered', YYYYMMDD, df_Con_flt1.shape)\n",
    "\n",
    "# Out2 = DirTransTU + '/' + GTFS_TU_Prefix + '_' + FileId + \"_\" + Agency + '_NoROT_' + str(YYYYMMDD) + '_TID' + str(TripID) + '.csv'\n",
    "Out2 = DirTransTU + '/' + GTFS_TU_Prefix + '_' + FileId + \"_\" + Agency + '_NoROT_' + YYYYMMDD + '_TID' + TripID + '.csv'\n",
    "\n",
    "df_Con_flt2 = df_Con_Flt.loc[df_Con_Flt['trip_update.trip.trip_id'] == TripID]\n",
    "print('Filtered', TripID, df_Con_flt2.shape)\n",
    "df_Con_flt2.to_csv(Out2, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p style=\"font-weight:bold; font-size:18px; color:black; line-height:normal; margin:0px; padding:4px; background-color:yellow\">\n",
    "    TEST: XXXXXXXX\n",
    "</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Back to SCALUT TfNSW GTFS <b>[Table of Contents](SCALUT_TfNSW_GTFS_Analysis_TOC_v01.ipynb)</b>"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "GTFS_r_test.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
